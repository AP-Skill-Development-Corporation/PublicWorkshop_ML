{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification:\n",
    "\n",
    "* Classifies the data into groups.\n",
    "\n",
    "**When to use regression and classification?**\n",
    "\n",
    "* If we want to predict the output then we will use the regression algorithms.\n",
    "* If we want to classify the input data into different grous then we will use the classification algorithms.\n",
    "\n",
    "**Logistic Regression**:\n",
    "\n",
    "* This is one of the classification algorithm.\n",
    "* The classification done by using the two binary values 0 and 1.\n",
    "* The plot for the 0 and 1 will be in the format of sigmoid curve or s-curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Equation**:\n",
    "<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning2.png\">\n",
    "\n",
    "**Logistic Equation between 0 and 1**:\n",
    "<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning3.png\">\n",
    "\n",
    "**Logistic Regression between the range -infinity to infinity**:\n",
    "<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning4.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Logistic Regression:**\n",
    "\n",
    "* Binomial Regression -- only two categories eill be considered.\n",
    "* Multinomial Regression -- more than 3 unoredered categories.\n",
    "* ordinal Regression -- more than 3 ordered categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/AP-Skill-Development-Corporation/Tirumala-ML/main/Day-5/diabetes.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "Pregnancies                 768 non-null int64\n",
      "Glucose                     768 non-null int64\n",
      "BloodPressure               768 non-null int64\n",
      "SkinThickness               768 non-null int64\n",
      "Insulin                     768 non-null int64\n",
      "BMI                         768 non-null float64\n",
      "DiabetesPedigreeFunction    768 non-null float64\n",
      "Age                         768 non-null int64\n",
      "Outcome                     768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.notnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting the data into input and output:\n",
    "\n",
    "X = data.drop('Outcome',axis = 1)\n",
    "y = data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training and testing the model:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niharikaa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the Logistic Regression:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LogisticRegression in module sklearn.linear_model.logistic object:\n",
      "\n",
      "class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n",
      " |  entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n",
      " |  both dense and sparse input. Use C-ordered arrays or CSR matrices\n",
      " |  containing 64-bit floats for optimal performance; any other input format\n",
      " |  will be converted (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation. The 'liblinear' solver supports both L1 and L2\n",
      " |  regularization, with a dual formulation only for the L2 penalty.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : str, 'l1' or 'l2', default: 'l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default: False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default: 1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default: True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default 1.\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default: None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default: None\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      " |      'liblinear'.\n",
      " |  \n",
      " |  solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              default: 'liblinear'.\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      " |        'liblinear' and 'saga' handle L1 penalty.\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default: 100\n",
      " |      Useful only for the newton-cg, sag and lbfgs solvers.\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default: False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : array, shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : array, shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : array, shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
      " |  ...                          multi_class='multinomial').fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SGDClassifier : incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,) optional\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Log of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = lr.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,  11],\n",
       "       [ 26,  36]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Checking the accuracy of the model:\n",
    "\n",
    "from sklearn import metrics\n",
    "c_matrix = metrics.confusion_matrix(y_test,y_predict)\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8072916666666666\n",
      "recall: 0.5806451612903226\n",
      "precision: 0.7659574468085106\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\",metrics.accuracy_score(y_test,y_predict))\n",
    "print(\"recall:\",metrics.recall_score(y_test,y_predict))\n",
    "print(\"precision:\",metrics.precision_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8072916666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(119 + 36)/(119 + 11 + 26 +36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pre = lr.predict_proba(X_test)[::,1]\n",
    "fpr,tpr,_ = metrics.roc_curve(y_test,y_pre)\n",
    "a = metrics.roc_auc_score(y_test,y_pre)\n",
    "plt.plot(fpr,tpr,label = \"Data,auccuracy=\"+str(a))\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot for confucion matrix:\n",
    "\n",
    "class_name = [0,1]\n",
    "fig,ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_name))\n",
    "plt.xticks(tick_marks,class_name)\n",
    "plt.yticks(tick_marks,class_name)\n",
    "sns.heatmap(pd.DataFrame(c_matrix),annot = True,cmap = \"Purples\",fmt = 'g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
